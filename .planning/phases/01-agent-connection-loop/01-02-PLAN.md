---
phase: 01-agent-connection-loop
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - hivemind/pipeline/__init__.py
  - hivemind/pipeline/pii.py
  - hivemind/pipeline/embedder.py
autonomous: true
requirements:
  - TRUST-01
  - KM-08

must_haves:
  truths:
    - "PII stripping detects and replaces emails, phone numbers, names, addresses with typed placeholders"
    - "API keys, tokens, passwords, connection strings, JWTs are detected and replaced with [API_KEY] or [REDACTED]"
    - "Content with >50% redacted tokens is auto-rejected (should_reject=True)"
    - "Typed placeholders used when entity type is confident ([EMAIL], [PHONE], [NAME], [LOCATION], [API_KEY]), [REDACTED] as fallback"
    - "Embedding provider wraps sentence-transformers behind an abstract interface"
    - "Embedding model name and dimensions are queryable from the provider"
  artifacts:
    - path: "hivemind/pipeline/pii.py"
      provides: "PII stripping pipeline: Presidio + GLiNER + custom API key recognizers"
      exports: ["PIIPipeline", "strip_pii"]
      min_lines: 80
    - path: "hivemind/pipeline/embedder.py"
      provides: "Embedding model abstraction with SentenceTransformer implementation"
      exports: ["EmbeddingProvider", "SentenceTransformerProvider"]
      min_lines: 40
  key_links:
    - from: "hivemind/pipeline/pii.py"
      to: "presidio_analyzer.AnalyzerEngine"
      via: "Presidio analyzer with GLiNER + custom recognizers"
      pattern: "AnalyzerEngine"
    - from: "hivemind/pipeline/pii.py"
      to: "presidio_anonymizer.AnonymizerEngine"
      via: "Anonymizer with typed operator config"
      pattern: "AnonymizerEngine"
    - from: "hivemind/pipeline/embedder.py"
      to: "sentence_transformers.SentenceTransformer"
      via: "Model loaded once at init"
      pattern: "SentenceTransformer"
---

<objective>
Build the PII stripping pipeline and embedding provider as standalone modules with no database dependency. These are pure-function modules that transform text: PII pipeline takes raw content and returns cleaned content, embedding provider takes text and returns vector.

Purpose: The PII pipeline is the trust foundation -- every piece of knowledge must pass through it before touching any storage. The embedding provider enables semantic search. Both are needed by `add_knowledge` and `search_knowledge` tools in Plan 03.
Output: Two importable modules: `hivemind.pipeline.pii` (PII detection + anonymization) and `hivemind.pipeline.embedder` (embedding generation with model abstraction).
</objective>

<execution_context>
@/Users/amirkellousidhoum/.claude/get-shit-done/workflows/execute-plan.md
@/Users/amirkellousidhoum/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-agent-connection-loop/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: PII stripping pipeline with Presidio + GLiNER + API key detection</name>
  <files>
    hivemind/pipeline/__init__.py
    hivemind/pipeline/pii.py
  </files>
  <action>
Create the multi-layer PII stripping pipeline:

**hivemind/pipeline/pii.py:**

`class PIIPipeline`:
- Singleton pattern: `_instance` class variable, `get_instance()` classmethod
- `__init__()`:
  1. Create `AnalyzerEngine()`
  2. Create and register `GLiNERRecognizer` with model `knowledgator/gliner-pii-base-v1.0`:
     - Entity mapping from research Pattern 4: name->PERSON, email address->EMAIL_ADDRESS, phone number->PHONE_NUMBER, location address->LOCATION, password->PASSWORD, username->USERNAME, credit card number->CREDIT_CARD, date of birth->DATE_TIME, social security number->US_SSN, driver's license->US_DRIVER_LICENSE, passport number->US_PASSPORT, bank account number->US_BANK_NUMBER, ip address->IP_ADDRESS, medical record number->MEDICAL_LICENSE, health insurance id->MEDICAL_LICENSE
     - `flat_ner=False`, `multi_label=True`, `map_location="cpu"`
  3. Create and register custom `PatternRecognizer` for API keys/secrets with patterns:
     - AWS key: `r"AKIA[0-9A-Z]{16}"` score 0.9
     - GitHub token (classic): `r"ghp_[A-Za-z0-9]{36}"` score 0.9
     - GitHub token (fine-grained): `r"github_pat_[A-Za-z0-9_]{82}"` score 0.9
     - Google API key: `r"AIza[0-9A-Za-z\-_]{35}"` score 0.9
     - Stripe key: `r"(?:sk|pk)_(?:test|live)_[A-Za-z0-9]{24,}"` score 0.9
     - Slack token: `r"xox[baprs]-[A-Za-z0-9-]+"` score 0.85
     - JWT: `r"eyJ[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+"` score 0.85
     - RSA private key: `r"-----BEGIN (?:RSA )?PRIVATE KEY-----"` score 0.95
     - Generic secret assignment: `r"(?i)(?:api[_-]?key|secret[_-]?key|access[_-]?token|auth[_-]?token|password|passwd|pwd)\s*[:=]\s*['\"]?\S{8,}['\"]?"` score 0.7
     - Connection string: `r"(?i)(?:postgres(?:ql)?|mysql|mongodb|redis|amqp)://\S+"` score 0.9
     - Private URL (localhost/internal): `r"(?:https?://)?(?:localhost|127\.0\.0\.1|10\.\d+\.\d+\.\d+|192\.168\.\d+\.\d+|172\.(?:1[6-9]|2\d|3[01])\.\d+\.\d+)(?::\d+)?(?:/\S*)?"` score 0.7
  4. Create `AnonymizerEngine()`
  5. Define operator config dict with typed placeholders per user decision:
     - EMAIL_ADDRESS -> `[EMAIL]`
     - PHONE_NUMBER -> `[PHONE]`
     - PERSON -> `[NAME]`
     - LOCATION -> `[LOCATION]`
     - API_KEY -> `[API_KEY]`
     - CREDIT_CARD -> `[CREDIT_CARD]`
     - US_SSN -> `[REDACTED]`
     - IP_ADDRESS -> `[IP_ADDRESS]`
     - PASSWORD -> `[REDACTED]`
     - USERNAME -> `[USERNAME]`
     - DEFAULT -> `[REDACTED]`

- `strip(self, text: str) -> tuple[str, bool]`:
  1. Run `analyzer.analyze(text=text, language="en")` to get entity results
  2. Run `anonymizer.anonymize(text=text, analyzer_results=results, operators=self.operators)` to get anonymized text
  3. Calculate rejection ratio: count placeholder tokens matching `r'\[(?:EMAIL|PHONE|NAME|LOCATION|API_KEY|CREDIT_CARD|IP_ADDRESS|USERNAME|REDACTED)\]'` in anonymized text, divide by `max(len(anonymized_text.split()), 1)`
  4. `should_reject = ratio > 0.50` (per user decision: >50% threshold)
  5. Return `(anonymized_text, should_reject)`

Module-level convenience function:
```python
def strip_pii(text: str) -> tuple[str, bool]:
    return PIIPipeline.get_instance().strip(text)
```

IMPORTANT per user decision:
- PII stripping is SILENT -- no logging of what was detected, no before/after comparison
- PII stripping happens BEFORE any storage -- the raw text must never be persisted
- Code snippets in content are stripped too -- safety first
  </action>
  <verify>
Run `python -c "from hivemind.pipeline.pii import PIIPipeline; print('PII module importable')"`.
Note: Full functional test requires spacy model (`python -m spacy download en_core_web_sm`) and GLiNER model download. These will be verified during integration. The import test confirms the module structure is correct.
  </verify>
  <done>PIIPipeline class exists as singleton with Presidio + GLiNER + custom API key/secret recognizers. strip() method returns (cleaned_text, should_reject) tuple. Typed placeholders used for confident entity types, [REDACTED] as fallback. 50% rejection threshold implemented. Module-level strip_pii convenience function available.</done>
</task>

<task type="auto">
  <name>Task 2: Embedding provider abstraction with SentenceTransformer implementation</name>
  <files>hivemind/pipeline/embedder.py</files>
  <action>
Create the embedding model abstraction layer (KM-08):

**hivemind/pipeline/embedder.py:**

`class EmbeddingProvider(ABC)`:
- Abstract methods:
  - `embed(self, text: str) -> list[float]` — embed a single text
  - `embed_batch(self, texts: list[str]) -> list[list[float]]` — embed multiple texts
  - `model_id` property -> str (e.g., "sentence-transformers/all-MiniLM-L6-v2")
  - `model_revision` property -> str | None (HuggingFace commit hash)
  - `dimensions` property -> int (embedding vector size)

`class SentenceTransformerProvider(EmbeddingProvider)`:
- `__init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2")`:
  - Load model via `SentenceTransformer(model_name)`
  - Store model name
  - Detect dimensions from model: `self._dimensions = self._model.get_sentence_embedding_dimension()`
  - Get revision from model card or huggingface_hub if available, else None
- `embed(self, text: str) -> list[float]`:
  - `return self._model.encode(text, normalize_embeddings=True).tolist()`
  - Note: `normalize_embeddings=True` ensures cosine similarity works correctly with pgvector's cosine_distance
- `embed_batch(self, texts: list[str]) -> list[list[float]]`:
  - `return [e.tolist() for e in self._model.encode(texts, normalize_embeddings=True)]`
- Properties return stored values

`class _EmbedderSingleton`:
- Module-level singleton holder (similar to PIIPipeline pattern)
- `_instance: EmbeddingProvider | None = None`
- `get_embedder(model_name: str | None = None) -> EmbeddingProvider` function:
  - If instance exists, return it
  - Otherwise create `SentenceTransformerProvider(model_name or settings.embedding_model)` and cache
  - Import settings lazily to avoid circular imports

This abstraction enables:
1. Swapping to a different embedding model by implementing EmbeddingProvider
2. Storing the model_id and model_revision in deployment_config at startup (KM-08)
3. Detecting if the deployed model differs from what was used to generate existing embeddings
  </action>
  <verify>
Run `python -c "from hivemind.pipeline.embedder import EmbeddingProvider, SentenceTransformerProvider; print('Embedder module importable')"`.
Note: Full functional test requires sentence-transformers model download. Import test confirms module structure.
  </verify>
  <done>EmbeddingProvider ABC defined with embed/embed_batch/model_id/model_revision/dimensions. SentenceTransformerProvider implements it with all-MiniLM-L6-v2. Singleton accessor get_embedder() available. Model revision tracking enabled for KM-08 compliance.</done>
</task>

</tasks>

<verification>
1. `from hivemind.pipeline.pii import PIIPipeline, strip_pii` imports without error
2. `from hivemind.pipeline.embedder import EmbeddingProvider, SentenceTransformerProvider, get_embedder` imports without error
3. PIIPipeline has patterns for AWS keys, GitHub tokens, Stripe keys, Slack tokens, JWTs, RSA keys, generic secrets, connection strings, private URLs
4. Operator config produces typed placeholders: [EMAIL], [PHONE], [NAME], [LOCATION], [API_KEY], [CREDIT_CARD], [IP_ADDRESS], [USERNAME], [REDACTED]
5. 50% rejection threshold calculated as placeholder_count / total_token_count
6. EmbeddingProvider is abstract with embed, embed_batch, model_id, model_revision, dimensions
7. SentenceTransformerProvider normalizes embeddings for correct cosine similarity
</verification>

<success_criteria>
- Both modules import cleanly without database dependencies
- PIIPipeline covers all PII types from TRUST-01: Presidio entities, GLiNER zero-shot, API secret patterns, private URL detection
- Embedding provider abstraction satisfies KM-08: model is pinnable, revision is trackable, dimensions are queryable
- No deferred features included (no French-specific identifiers, no feedback loop)
</success_criteria>

<output>
After completion, create `.planning/phases/01-agent-connection-loop/01-02-SUMMARY.md`
</output>
