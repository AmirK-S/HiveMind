---
phase: 01-agent-connection-loop
plan: 03
type: execute
wave: 2
depends_on:
  - "01-01"
  - "01-02"
files_modified:
  - hivemind/server/__init__.py
  - hivemind/server/main.py
  - hivemind/server/auth.py
  - hivemind/server/tools/__init__.py
  - hivemind/server/tools/add_knowledge.py
  - hivemind/server/tools/search_knowledge.py
autonomous: true
requirements:
  - MCP-01
  - MCP-02
  - MCP-03
  - ACL-01

must_haves:
  truths:
    - "An agent can connect to HiveMind via MCP Streamable HTTP transport"
    - "An agent calling add_knowledge gets content PII-stripped, then queued in pending_contributions (never the commons)"
    - "add_knowledge returns immediately with contribution_id and status='queued' — agent moves on"
    - "Content that is >50% redacted after PII stripping is auto-rejected with an isError response"
    - "An agent calling search_knowledge gets summary-tier results (id, title, category, confidence, org_attribution, relevance_score)"
    - "An agent can request full content for a specific knowledge item by ID"
    - "All queries are scoped to org_id extracted from bearer token — never from tool arguments"
    - "Search returns results from both the agent's private namespace and public commons"
  artifacts:
    - path: "hivemind/server/main.py"
      provides: "FastMCP server with Streamable HTTP, lifespan init for PII pipeline + embedder + deployment_config"
      contains: "create_streamable_http_app"
      min_lines: 40
    - path: "hivemind/server/auth.py"
      provides: "Bearer token auth extracting org_id and agent_id from JWT"
      exports: ["get_auth_context", "AuthContext"]
      min_lines: 20
    - path: "hivemind/server/tools/add_knowledge.py"
      provides: "add_knowledge MCP tool: PII strip -> hash -> quarantine insert"
      contains: "add_knowledge"
      min_lines: 50
    - path: "hivemind/server/tools/search_knowledge.py"
      provides: "search_knowledge MCP tool: embed query -> pgvector cosine search -> tiered response"
      contains: "search_knowledge"
      min_lines: 60
  key_links:
    - from: "hivemind/server/main.py"
      to: "hivemind/pipeline/pii.py"
      via: "PIIPipeline.get_instance() at startup lifespan"
      pattern: "PIIPipeline"
    - from: "hivemind/server/main.py"
      to: "hivemind/pipeline/embedder.py"
      via: "get_embedder() at startup lifespan"
      pattern: "get_embedder"
    - from: "hivemind/server/tools/add_knowledge.py"
      to: "hivemind/pipeline/pii.py"
      via: "strip_pii() before DB insert"
      pattern: "strip_pii"
    - from: "hivemind/server/tools/add_knowledge.py"
      to: "hivemind/db/models.py"
      via: "PendingContribution insert"
      pattern: "PendingContribution"
    - from: "hivemind/server/tools/search_knowledge.py"
      to: "hivemind/pipeline/embedder.py"
      via: "get_embedder().embed(query)"
      pattern: "get_embedder.*embed"
    - from: "hivemind/server/tools/search_knowledge.py"
      to: "hivemind/db/models.py"
      via: "KnowledgeItem cosine_distance query"
      pattern: "cosine_distance"
---

<objective>
Build the MCP server with Streamable HTTP transport, bearer token authentication, and the two core tools: `add_knowledge` (contribute with PII stripping) and `search_knowledge` (semantic search with tiered response). This creates the contribute/retrieve loop -- the heart of HiveMind.

Purpose: This is the core product loop. An agent connects, contributes knowledge that gets PII-stripped and queued, and can search the commons for existing knowledge. Without this, nothing works.
Output: A running MCP server at `/mcp` with two functional tools, JWT-based org isolation, and integration with the PII pipeline and embedding provider from Plan 02.
</objective>

<execution_context>
@/Users/amirkellousidhoum/.claude/get-shit-done/workflows/execute-plan.md
@/Users/amirkellousidhoum/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-agent-connection-loop/01-RESEARCH.md
@.planning/phases/01-agent-connection-loop/01-01-SUMMARY.md
@.planning/phases/01-agent-connection-loop/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: MCP server with Streamable HTTP + auth + add_knowledge tool</name>
  <files>
    hivemind/server/__init__.py
    hivemind/server/main.py
    hivemind/server/auth.py
    hivemind/server/tools/__init__.py
    hivemind/server/tools/add_knowledge.py
  </files>
  <action>
Build the MCP server foundation and the `add_knowledge` tool:

**hivemind/server/auth.py:**
- `@dataclass class AuthContext`: `org_id: str`, `agent_id: str`
- `def decode_token(token: str) -> AuthContext`:
  - Decode JWT using `jose.jwt.decode(token, settings.secret_key, algorithms=["HS256"])`
  - Extract `org_id` and `agent_id` claims
  - Raise ValueError if either claim is missing
- `def create_token(org_id: str, agent_id: str) -> str`:
  - Utility to create tokens (for testing and CLI use)
  - `jose.jwt.encode({"org_id": org_id, "agent_id": agent_id}, settings.secret_key, algorithm="HS256")`
- IMPORTANT per research anti-pattern: NEVER trust org_id from tool arguments. Always extract from token.

**hivemind/server/main.py:**
- Create `FastMCP("HiveMind")` instance
- Import and register all tool functions (add_knowledge, search_knowledge, list_knowledge, delete_knowledge)
  - For list_knowledge and delete_knowledge: create placeholder stub functions that return `{"error": "Not yet implemented"}` -- Plan 04 will replace these
- `lifespan` async context manager:
  1. Initialize PIIPipeline singleton (triggers GLiNER model load)
  2. Initialize embedding provider singleton via `get_embedder()`
  3. Store deployment config in DB: embedding model name + revision in `deployment_config` table (KM-08)
     - On first startup: INSERT model_name and model_revision
     - On subsequent startups: SELECT and compare — log warning if model changed (but don't block)
  4. Yield (server runs)
  5. Cleanup: dispose async engine
- Create the ASGI app:
  ```python
  app = create_streamable_http_app(
      server=mcp,
      streamable_http_path="/mcp",
      stateless_http=True,
      json_response=True,
  )
  ```
- Add a `/health` endpoint on the FastAPI app for simple health checks
- Entry point: `uvicorn hivemind.server.main:app --host 0.0.0.0 --port 8000`

**hivemind/server/tools/add_knowledge.py:**
- Define Pydantic model `AddKnowledgeInput`:
  - `content: str` (required, min_length=10)
  - `category: str` (required, must be valid KnowledgeCategory value)
  - `confidence: float = 0.8` (0.0 to 1.0)
  - `framework: str | None = None`
  - `language: str | None = None`
  - `version: str | None = None`
  - `tags: list[str] | None = None`
  - `run_id: str | None = None`

- Function `add_knowledge` registered as `@mcp.tool()`:
  - Signature accepts all AddKnowledgeInput fields as individual parameters (FastMCP maps them)
  - Extract auth context: parse the Authorization header from the MCP request context
    - FastMCP provides request context via `mcp.get_context()` — use it to access headers
    - Decode bearer token via `decode_token()` to get org_id and agent_id
  - Step 1: PII strip the content via `strip_pii(content)`
  - Step 2: If `should_reject` is True, return error response:
    ```python
    return CallToolResult(
        content=[TextContent(type="text", text="Rejected: too much content was identified as sensitive and redacted (>50%). The contribution cannot be meaningfully preserved.")],
        isError=True,
    )
    ```
  - Step 3: Compute `content_hash = hashlib.sha256(cleaned_content.encode()).hexdigest()`
  - Step 4: Validate category against KnowledgeCategory enum; return isError if invalid
  - Step 5: Insert into `pending_contributions` table:
    - org_id from auth context
    - source_agent_id from auth context
    - content = cleaned (PII-stripped) text
    - content_hash = SHA-256 of cleaned text
    - All metadata fields from input
    - contributed_at = utcnow
  - Step 6: Return success:
    ```python
    return {
        "contribution_id": str(contribution.id),
        "status": "queued",
        "category": category,
        "message": "Knowledge contribution queued for review."
    }
    ```
  - CRITICAL per user decision: PII stripping happens BEFORE the DB insert. The raw content must NEVER be stored, not even in the pending queue.

Note on auth context extraction: FastMCP v2 may not directly expose HTTP headers in tool functions. Research the `mcp.get_context()` API. If headers are not accessible in tool context, implement auth as FastAPI middleware that stores AuthContext in a context variable (contextvars.ContextVar), accessible from tool functions. Document the chosen approach.
  </action>
  <verify>
Run `python -c "from hivemind.server.main import app; print('Server importable')"` — confirms the ASGI app can be constructed.
Run `python -c "from hivemind.server.auth import AuthContext, decode_token, create_token; print('Auth OK')"`.
Run `python -c "from hivemind.server.tools.add_knowledge import add_knowledge; print('add_knowledge importable')"`.
  </verify>
  <done>MCP server creates ASGI app with Streamable HTTP at /mcp. Auth extracts org_id + agent_id from bearer JWT (never from tool args). add_knowledge tool: validates input, strips PII, auto-rejects >50% redacted, computes content_hash, inserts to pending_contributions, returns contribution_id with status='queued'. Lifespan initializes PII pipeline + embedder + deployment config. Health endpoint at /health.</done>
</task>

<task type="auto">
  <name>Task 2: search_knowledge tool with tiered response and vector search</name>
  <files>hivemind/server/tools/search_knowledge.py</files>
  <action>
Build the `search_knowledge` MCP tool with summary-first response pattern:

**hivemind/server/tools/search_knowledge.py:**

The tool supports two modes:
1. **Search mode** (default): query text -> embed -> cosine similarity search -> return summary-tier results
2. **Fetch mode**: id provided -> return full content for a specific knowledge item

Parameters:
- `query: str | None = None` — search text (required for search mode)
- `id: str | None = None` — specific item ID (for fetch mode / full content retrieval)
- `category: str | None = None` — filter by category
- `limit: int = 10` — max results (capped at settings.max_search_limit)
- `cursor: str | None = None` — pagination cursor (offset-based for Phase 1, encode as base64 int)

Logic:

**Fetch mode (id provided):**
- Query `knowledge_items` by id
- Filter: `WHERE id = :id AND (org_id = :org_id OR is_public = True)`
  - Agents can only fetch items in their org OR public items
  - Return 404 if not found (don't reveal existence in other orgs per research pitfall 6)
- Return full content:
  ```python
  {
      "id": str(item.id),
      "content": item.content,
      "category": item.category.value,
      "confidence": item.confidence,
      "framework": item.framework,
      "language": item.language,
      "version": item.version,
      "tags": item.tags,
      "org_attribution": item.org_id,
      "contributed_at": item.contributed_at.isoformat(),
  }
  ```

**Search mode (query provided):**
- Embed the query: `query_embedding = get_embedder().embed(query)`
- Build SQLAlchemy query:
  ```python
  stmt = (
      select(
          KnowledgeItem,
          KnowledgeItem.embedding.cosine_distance(query_embedding).label("distance")
      )
      .where(
          (KnowledgeItem.org_id == org_id) | (KnowledgeItem.is_public == True)
      )
      .where(KnowledgeItem.embedding.isnot(None))  # skip items without embeddings
  )
  ```
- If `category` filter provided: add `.where(KnowledgeItem.category == category)`
- Order by distance ascending (closest = most relevant)
- Apply pagination: `.limit(limit).offset(offset_from_cursor)`
- Count total matches for pagination metadata
- Build summary-tier response (~30-50 tokens per result per user decision):
  ```python
  {
      "results": [
          {
              "id": str(item.id),
              "title": item.content[:80] + ("..." if len(item.content) > 80 else ""),
              "category": item.category.value,
              "confidence": item.confidence,
              "org_attribution": item.org_id,
              "relevance_score": round(1 - distance, 4),  # convert distance to similarity
          }
          for item, distance in results
      ],
      "total_found": total_count,
      "next_cursor": encode_cursor(offset + limit) if has_more else None,
  }
  ```

- If neither `query` nor `id` provided: return isError with message "Provide either 'query' for search or 'id' to fetch a specific item."

Cursor encoding: simple base64 of offset integer. `encode_cursor(offset) -> str`, `decode_cursor(cursor) -> int`.

Auth: Extract org_id from auth context (same pattern as add_knowledge).
  </action>
  <verify>
Run `python -c "from hivemind.server.tools.search_knowledge import search_knowledge; print('search_knowledge importable')"`.
Verify the function signature accepts query, id, category, limit, cursor parameters.
  </verify>
  <done>search_knowledge tool supports two modes: search (query -> embed -> cosine similarity -> summary tier) and fetch (id -> full content). Results scoped to agent's org + public commons. Summary tier returns ~30-50 tokens per result (id, title, category, confidence, org_attribution, relevance_score). Cursor-based pagination. Org isolation enforced via auth context, not tool arguments.</done>
</task>

</tasks>

<verification>
1. MCP server creates ASGI app bound to `/mcp` endpoint
2. `stateless_http=True` set for horizontal scaling
3. Auth decodes JWT and extracts org_id — tool arguments never trusted for org_id
4. add_knowledge: PII strips BEFORE any storage, auto-rejects >50% redacted
5. add_knowledge: inserts to pending_contributions (quarantine), NOT knowledge_items (commons)
6. add_knowledge: returns contribution_id + status='queued' (agent moves on)
7. search_knowledge: embeds query and uses cosine_distance for ranking
8. search_knowledge: returns summary tier by default (~30-50 tokens per result)
9. search_knowledge: fetch mode returns full content for specific ID
10. search_knowledge: scoped to `org_id = :org_id OR is_public = True`
11. Lifespan initializes PII pipeline + embedder + stores deployment config
</verification>

<success_criteria>
- Server app importable and constructable
- add_knowledge registered as MCP tool with correct parameter types
- search_knowledge registered as MCP tool supporting both search and fetch modes
- Auth context extraction works (token decode -> org_id + agent_id)
- Phase 1 success criteria 1 and 3 are satisfied (contribute enters quarantine, search returns ranked results)
</success_criteria>

<output>
After completion, create `.planning/phases/01-agent-connection-loop/01-03-SUMMARY.md`
</output>
