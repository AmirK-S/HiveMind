---
phase: 04-dashboard-distribution
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - hivemind/api/routes/stream.py
  - hivemind/api/routes/contributions.py
  - hivemind/api/routes/stats.py
  - hivemind/api/router.py
  - hivemind/server/main.py
autonomous: true
requirements:
  - DASH-01
  - DASH-03
  - DASH-05
  - DASH-06

must_haves:
  truths:
    - "GET /api/v1/stream/feed returns a text/event-stream response with named events (public, private, ping)"
    - "POST /api/v1/contributions/{id}/approve moves a pending contribution to an approved knowledge item"
    - "POST /api/v1/contributions/{id}/reject removes a pending contribution from the queue"
    - "GET /api/v1/stats/commons returns total items, growth rate, retrieval volume, and domains covered"
    - "GET /api/v1/stats/org returns per-org contribution and retrieval statistics"
    - "GET /api/v1/stats/user returns per-user contribution counts and reciprocity metrics"
  artifacts:
    - path: "hivemind/api/routes/stream.py"
      provides: "SSE feed endpoint with PostgreSQL LISTEN/NOTIFY"
      exports: ["stream_router"]
    - path: "hivemind/api/routes/contributions.py"
      provides: "Approve and reject endpoints for pending contributions"
      exports: ["contributions_router"]
    - path: "hivemind/api/routes/stats.py"
      provides: "Commons health, org, and user statistics endpoints"
      exports: ["stats_router"]
  key_links:
    - from: "hivemind/api/routes/stream.py"
      to: "asyncpg LISTEN/NOTIFY"
      via: "asyncpg.connect() with add_listener on knowledge_published channel"
      pattern: "add_listener.*knowledge_published"
    - from: "hivemind/api/routes/contributions.py"
      to: "hivemind/db/models.py"
      via: "PendingContribution and KnowledgeItem ORM models"
      pattern: "PendingContribution|KnowledgeItem"
    - from: "hivemind/api/router.py"
      to: "hivemind/api/routes/stream.py"
      via: "api_router.include_router(stream_router)"
      pattern: "include_router.*stream_router"
---

<objective>
Add three new FastAPI route modules to the REST API layer: (1) SSE streaming endpoint for real-time knowledge feeds, (2) contribution approval/rejection endpoints for dashboard review workflow, and (3) statistics endpoints for commons health metrics and reciprocity ledger.

Purpose: The web dashboard (Plans 04-02 and 04-03) consumes these endpoints. Without them, there is no data source for live feeds, approval actions, or analytics.
Output: Three new route files registered in the API router, all authenticated via require_api_key.
</objective>

<execution_context>
@/Users/amirkellousidhoum/.claude/get-shit-done/workflows/execute-plan.md
@/Users/amirkellousidhoum/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@hivemind/api/router.py
@hivemind/api/auth.py
@hivemind/api/routes/knowledge.py
@hivemind/api/routes/outcomes.py
@hivemind/server/main.py
@hivemind/db/models.py
@hivemind/cli/client.py
@hivemind/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: SSE stream endpoint with PostgreSQL LISTEN/NOTIFY</name>
  <files>hivemind/api/routes/stream.py</files>
  <action>
Create `hivemind/api/routes/stream.py` with a `stream_router` APIRouter (prefix="/stream", tags=["stream"]).

Implement `GET /stream/feed` endpoint:
1. Authenticate via `require_api_key` dependency — extract `org_id` from the returned ApiKey record.
2. Open a **dedicated** asyncpg connection using `asyncpg.connect(settings.database_url)` — NOT through SQLAlchemy session (Pitfall 2 from research: LISTEN/NOTIFY requires a persistent idle connection, not a pooled transactional session).
3. Use `sse-starlette` `EventSourceResponse` with a `ping` interval of 25 seconds (anti-pattern: SSE without keepalives causes proxy timeouts at 60s).
4. Create an async generator that:
   - Sets up `asyncio.Queue` and registers listener on `knowledge_published` channel via `conn.add_listener()`.
   - Loops, awaiting queue items with 30s timeout (yields `ping` event on timeout for keepalive).
   - Parses each payload as JSON. Routes to event type: `"public"` if `is_public=True`, `"private"` otherwise.
   - For `"private"` events, only yields if `item["org_id"] == str(org_id)` — namespace isolation.
   - For `"public"` events, yields to all connected clients.
   - In `finally` block: removes listener and closes the dedicated asyncpg connection.
5. The database_url used for asyncpg.connect() must strip the `+asyncpg` dialect suffix if present (SQLAlchemy uses `postgresql+asyncpg://...` but raw asyncpg needs `postgresql://...`).

Add `sse-starlette>=3.2.0` to pyproject.toml dependencies. Use `from sse_starlette.sse import EventSourceResponse`.

Also add a NOTIFY trigger helper: create a standalone async function `notify_knowledge_published(session, item_data: dict)` that executes `SELECT pg_notify('knowledge_published', :payload)` with a JSON-serialized payload containing id, is_public, org_id, category, title. This function will be called by the contributions approve endpoint (Task 2) after inserting a KnowledgeItem.
  </action>
  <verify>
`python -c "from hivemind.api.routes.stream import stream_router, notify_knowledge_published"` succeeds without import errors. Verify `sse-starlette` is in pyproject.toml.
  </verify>
  <done>SSE endpoint at /api/v1/stream/feed returns EventSourceResponse with public/private named events, 25s ping keepalive, dedicated asyncpg connection for LISTEN/NOTIFY, and a reusable notify helper.</done>
</task>

<task type="auto">
  <name>Task 2: Contribution approve/reject REST endpoints</name>
  <files>hivemind/api/routes/contributions.py</files>
  <action>
Create `hivemind/api/routes/contributions.py` with a `contributions_router` APIRouter (prefix="/contributions", tags=["contributions"]).

Implement two endpoints, both authenticated via `require_api_key`:

**POST /contributions/{contribution_id}/approve:**
1. Extract org_id from the ApiKey record.
2. Query `PendingContribution` by id, filter by `org_id` for namespace isolation. Return 404 if not found (404-not-403 pattern from Phase 2).
3. Mirror the approval logic from `hivemind/cli/client.py` `approve_contribution()`:
   - Generate embedding for the contribution content using `get_embedder().embed()`.
   - Create a `KnowledgeItem` from the PendingContribution fields (title, content, category, org_id, source_agent_id, content_hash, embedding, contributed_at=now).
   - Set `quality_score=0.5` (neutral prior from Phase 3).
   - Delete the PendingContribution.
   - After commit: call `notify_knowledge_published()` to emit a PostgreSQL NOTIFY with the new item's data.
   - After commit: dispatch webhooks via `dispatch_webhooks(org_id, "knowledge.approved", item_data)` (best-effort, never blocks — same pattern as CLI approval).
4. Return 200 with `{"status": "approved", "item_id": str(item.id)}`.

**POST /contributions/{contribution_id}/reject:**
1. Extract org_id from the ApiKey record.
2. Query `PendingContribution` by id, filter by `org_id`. Return 404 if not found.
3. Delete the PendingContribution.
4. After commit: dispatch webhooks with event_type "knowledge.rejected".
5. Return 200 with `{"status": "rejected", "contribution_id": str(contribution_id)}`.

**GET /contributions (list pending):**
1. Query `PendingContribution` filtered by `org_id`, ordered by `created_at DESC`.
2. Return paginated list with `limit` (default 50) and `offset` (default 0) query params.
3. Each item includes: id, title, content (truncated to 500 chars for list view), category, source_agent_id, created_at, content_hash.

Use `from hivemind.api.routes.stream import notify_knowledge_published` for the NOTIFY call. Use `from hivemind.webhooks.tasks import dispatch_webhooks` for webhook dispatch. Use `from hivemind.pipeline.embedder import get_embedder` for embedding generation.

Set explicit `operation_id` on each route for clean SDK generation (pattern from Phase 3).
  </action>
  <verify>
`python -c "from hivemind.api.routes.contributions import contributions_router"` succeeds. Verify all three endpoints have explicit operation_id set.
  </verify>
  <done>Approve endpoint creates KnowledgeItem from PendingContribution with embedding, fires NOTIFY + webhook. Reject endpoint deletes PendingContribution and fires webhook. List endpoint returns paginated pending contributions for the org.</done>
</task>

<task type="auto">
  <name>Task 3: Statistics endpoints + router wiring</name>
  <files>hivemind/api/routes/stats.py, hivemind/api/router.py, hivemind/server/main.py</files>
  <action>
Create `hivemind/api/routes/stats.py` with a `stats_router` APIRouter (prefix="/stats", tags=["stats"]).

Implement three endpoints, all authenticated via `require_api_key`:

**GET /stats/commons (DASH-06):**
1. Query aggregate stats across all orgs (public commons view):
   - `total_items`: COUNT of knowledge_items WHERE deleted_at IS NULL
   - `total_pending`: COUNT of pending_contributions
   - `growth_rate_24h`: COUNT of knowledge_items WHERE contributed_at > now - 24h
   - `growth_rate_7d`: COUNT of knowledge_items WHERE contributed_at > now - 7d
   - `retrieval_volume_24h`: SUM of retrieval_count WHERE last updated in 24h (or COUNT of quality_signals WHERE signal_type='retrieval' AND created_at > now - 24h)
   - `domains_covered`: COUNT DISTINCT category from knowledge_items
   - `categories`: list of {category, count} grouped by category
2. Return as JSON object.

**GET /stats/org (DASH-03):**
1. Filter by org_id from ApiKey record.
2. Return:
   - `contributions_total`: COUNT of knowledge_items for this org
   - `contributions_pending`: COUNT of pending_contributions for this org
   - `contributions_approved_24h`: COUNT approved in last 24h
   - `retrievals_by_others`: SUM of retrieval_count on items WHERE org_id = this org (how many times other agents retrieved this org's knowledge — the reciprocity metric)
   - `helpful_count`: SUM of helpful_count on items for this org
   - `not_helpful_count`: SUM of not_helpful_count on items for this org
   - `top_items`: Top 5 items by retrieval_count for this org (id, title, retrieval_count)

**GET /stats/user (DASH-03):**
1. Accept `agent_id` query parameter (optional — defaults to all agents in org).
2. Filter by org_id from ApiKey record.
3. Return:
   - `agent_contributions`: COUNT of knowledge_items WHERE source_agent_id = agent_id
   - `agent_retrievals_received`: SUM of retrieval_count on items WHERE source_agent_id = agent_id (reciprocity: how many times this agent's contributions were retrieved by others)
   - `agent_helpful_ratio`: helpful_count / (helpful_count + not_helpful_count) for this agent's items

All queries use existing ORM models. No new tables needed — all data is already tracked in knowledge_items and quality_signals from Phases 1-3.

**Wire up routers in `hivemind/api/router.py`:**
Add imports and `include_router` calls for stream_router, contributions_router, and stats_router. The router.py file currently includes knowledge_router and outcomes_router — add the three new ones after those.

**Update `hivemind/server/main.py` lifespan** (if needed):
No lifespan changes needed — asyncpg is already available, sse-starlette has no init requirements. However, ensure `sse-starlette` is listed in pyproject.toml (done in Task 1).
  </action>
  <verify>
`python -c "from hivemind.api.routes.stats import stats_router; from hivemind.api.router import api_router"` succeeds. Verify api_router now includes 5 sub-routers (knowledge, outcomes, stream, contributions, stats).
  </verify>
  <done>Three stats endpoints return commons health metrics, org-level reciprocity data, and per-user contribution stats. All three new routers (stream, contributions, stats) are wired into the api_router.</done>
</task>

</tasks>

<verification>
1. `python -c "from hivemind.api.router import api_router"` — no import errors
2. `grep -c "include_router" hivemind/api/router.py` returns 5 (knowledge, outcomes, stream, contributions, stats)
3. `grep "sse-starlette" pyproject.toml` confirms dependency added
4. All new route files have explicit operation_id on each endpoint
</verification>

<success_criteria>
- Five API route modules are registered under /api/v1/
- SSE endpoint streams public/private events via PostgreSQL LISTEN/NOTIFY with 25s keepalive
- Approve endpoint creates KnowledgeItem with embedding and fires NOTIFY + webhook
- Reject endpoint removes PendingContribution and fires webhook
- Stats endpoints return commons health, org reciprocity, and per-user metrics using existing DB data
</success_criteria>

<output>
After completion, create `.planning/phases/04-dashboard-distribution/04-01-SUMMARY.md`
</output>
