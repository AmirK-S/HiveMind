---
phase: 03-quality-intelligence-sdks
plan: 06
type: execute
wave: 3
depends_on: ["03-01", "03-04"]
files_modified:
  - hivemind/quality/distillation.py
  - hivemind/webhooks/tasks.py
autonomous: true
requirements:
  - QI-04
  - QI-05

must_haves:
  truths:
    - "Distillation runs as a Celery periodic task triggered by volume/conflict thresholds"
    - "Distillation merges confirmed duplicates into consolidated items"
    - "Distillation flags contradictions for human review"
    - "Distillation generates summaries for clusters of related knowledge"
    - "Generated summaries are re-scanned through PII pipeline"
    - "Quality pre-screening filters low-score items before they appear in the review queue"
    - "Provenance links are maintained for erasure propagation"
  artifacts:
    - path: "hivemind/quality/distillation.py"
      provides: "Sleep-time distillation Celery task"
      exports: ["run_distillation"]
    - path: "hivemind/webhooks/tasks.py"
      provides: "Celery Beat schedule with distillation every 30 minutes"
      contains: "distillation-every-30m"
  key_links:
    - from: "hivemind/quality/distillation.py"
      to: "hivemind/dedup/pipeline.py"
      via: "uses dedup pipeline to find duplicates for merging"
      pattern: "run_dedup_pipeline"
    - from: "hivemind/quality/distillation.py"
      to: "hivemind/pipeline/pii.py"
      via: "re-scans generated summaries through PII pipeline"
      pattern: "PIIPipeline"
    - from: "hivemind/quality/distillation.py"
      to: "hivemind/quality/scorer.py"
      via: "quality pre-screening uses compute_quality_score"
      pattern: "quality_score"
---

<objective>
Implement sleep-time distillation as a background job that merges duplicates, flags contradictions, generates summaries, and applies quality pre-screening.

Purpose: QI-04 requires background distillation triggered by volume thresholds. QI-05 requires duplicate merging, contradiction flagging, summary generation, and quality pre-screening before human review. Together, these make the commons self-maintaining — users review a filtered shortlist, not raw agent output.

Output: Distillation Celery task with PII re-scan, provenance tracking, and quality gate, running on a 30-minute Celery Beat schedule with threshold-based activation.
</objective>

<execution_context>
@/Users/amirkellousidhoum/.claude/get-shit-done/workflows/execute-plan.md
@/Users/amirkellousidhoum/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-quality-intelligence-sdks/03-RESEARCH.md

# Plan 01 creates quality infrastructure, Plan 04 creates dedup/conflict pipeline
@.planning/phases/03-quality-intelligence-sdks/03-01-SUMMARY.md
@.planning/phases/03-quality-intelligence-sdks/03-04-SUMMARY.md

@hivemind/webhooks/tasks.py
@hivemind/pipeline/pii.py
@hivemind/quality/scorer.py
@hivemind/dedup/pipeline.py
@hivemind/conflict/resolver.py
@hivemind/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Sleep-time distillation task with duplicate merging and summary generation</name>
  <files>
    hivemind/quality/distillation.py
  </files>
  <action>
    1. Create `hivemind/quality/distillation.py` with:

       - `def run_distillation() -> dict`:
         Main distillation function called by Celery task. Runs synchronously (Celery worker).
         Steps:

         **a. Threshold check (short-circuit if below threshold):**
         - Count pending contributions: `SELECT COUNT(*) FROM pending_contributions`
         - Count unresolved conflicts: `SELECT COUNT(*) FROM quality_signals WHERE signal_type = 'contradiction' AND created_at > :last_distillation_run`
         - Read thresholds from settings: `settings.distillation_volume_threshold`, `settings.distillation_conflict_threshold`
         - If both below threshold: return `{ status: "skipped", reason: "below threshold", pending_count, conflict_count }`

         **b. Duplicate merging:**
         - Query approved knowledge_items clustered by content_hash similarity.
         - For items identified as duplicates (same content_hash OR from prior dedup pipeline DUPLICATE results):
           - Keep the item with the highest quality_score as the "canonical" item.
           - Set `expired_at = now()` on the non-canonical duplicates (system-time invalidation).
           - Maintain provenance: add JSONB field `provenance_links` on the canonical item containing the IDs of merged items (for erasure propagation — if source data is deleted, all derived items can be found).
         - Track: `duplicates_merged: int`.

         **c. Contradiction flagging:**
         - For items with "contradiction" signals, group by topic (same category + similar embedding).
         - Flag contradiction clusters by recording a "contradiction_cluster" signal with metadata containing all conflicting item IDs.
         - Track: `contradictions_flagged: int`.

         **d. Summary generation (for clusters of 3+ related items):**
         - Find clusters of knowledge_items with high cosine similarity (distance < 0.3) in the same category.
         - For clusters with 3+ items, call LLM to generate a summary.
         - LLM prompt: "Summarize these related knowledge items into a single concise item that captures the key information. Preserve technical accuracy. Output only the summary text."
         - If LLM unavailable, skip summary generation (non-blocking).
         - **PII re-scan:** Run each generated summary through `PIIPipeline.get_instance().strip()` (QI-04 requirement). If PII is found and stripped, log a warning.
         - Store summaries as new knowledge_items with:
           - `category` from the cluster's shared category
           - `content_hash` computed from summary text
           - `quality_score = 0.6` (slightly above neutral — summaries are curated)
           - `tags = {"distilled": true, "source_item_ids": [list of source IDs]}`
           - `org_id` from the cluster's shared org
         - Maintain provenance: `source_item_ids` in tags enables erasure propagation (if any source is deleted, the distilled summary can be identified and re-evaluated).
         - Track: `summaries_generated: int`.

         **e. Quality pre-screening:**
         - Query pending_contributions.
         - For each pending item, compute a preliminary quality estimate using the dedup pipeline to check for duplicates and the scorer for initial scoring.
         - Mark items with very low preliminary quality (< 0.2) with `is_sensitive_flagged = True` and add metadata note "low_quality_prescreened".
         - These items still appear in the review queue but are visually flagged as low-quality.
         - Track: `items_prescreened: int, low_quality_filtered: int`.

         **f. Update last run timestamp:**
         - Store `distillation_last_run = now()` in deployment_config table.

         Return: `{ status: "completed", duplicates_merged, contradictions_flagged, summaries_generated, items_prescreened, low_quality_filtered, run_at }`.

       Use sync SessionFactory (from `hivemind.cli.client`) — same pattern as webhook tasks. Import PIIPipeline lazily inside the function to avoid heavy imports at module level.

    AVOID: Do NOT run distillation synchronously in the request path — this is a background-only task.
    AVOID: Do NOT delete merged duplicates — set expired_at (immutable audit trail per KM-05).
    AVOID: Do NOT generate summaries without PII re-scan — the LLM may surface PII from source items.
  </action>
  <verify>
    Run `cd /Users/amirkellousidhoum/Desktop/Code/HiveMind && python -c "
from hivemind.quality.distillation import run_distillation
print('Distillation imported OK')
import inspect
sig = inspect.signature(run_distillation)
print(f'Return type: {sig.return_annotation}')
"` — import succeeds.
  </verify>
  <done>
    Distillation task handles: threshold check -> duplicate merging (expire non-canonical) -> contradiction flagging -> summary generation (LLM + PII re-scan) -> quality pre-screening. Provenance links maintained via tags.source_item_ids for erasure propagation. Low-quality items flagged but not blocked in review queue.
  </done>
</task>

<task type="auto">
  <name>Task 2: Celery Beat schedule and distillation task registration</name>
  <files>
    hivemind/webhooks/tasks.py
  </files>
  <action>
    1. Add distillation Celery task to `hivemind/webhooks/tasks.py`:
       ```python
       @celery_app.task(name="hivemind.distill")
       def run_distillation_task():
           """Sleep-time distillation — called by Celery Beat every 30 minutes.

           Evaluates volume/conflict thresholds inside the task body and short-circuits
           if conditions are not met (Celery Beat only supports time-based triggering;
           condition logic must live in the task body — research Pitfall 6).
           """
           from hivemind.quality.distillation import run_distillation
           return run_distillation()
       ```

    2. Update `configure_celery()` to include distillation in the Beat schedule:
       ```python
       from celery.schedules import crontab

       celery_app.conf.beat_schedule = {
           "quality-signal-aggregation": {
               "task": "hivemind.aggregate_quality_signals",
               "schedule": crontab(minute="*/10"),  # every 10 minutes
           },
           "distillation-every-30m": {
               "task": "hivemind.distill",
               "schedule": crontab(minute="*/30"),  # every 30 minutes
           },
       }
       ```
       Note: The quality-signal-aggregation entry was added in Plan 05. If Plan 05 runs before Plan 06 (same wave), the beat_schedule dict should include both entries. If Plan 06 runs first, include both entries here and Plan 05 will find its entry already present.

    AVOID: Do NOT implement custom Celery Beat scheduler logic — the threshold check is inside the task body (research Pitfall 6). Beat only does time-based scheduling.
  </action>
  <verify>
    Run `cd /Users/amirkellousidhoum/Desktop/Code/HiveMind && python -c "
from hivemind.webhooks.tasks import celery_app, configure_celery
configure_celery('redis://localhost:6379/0')
schedule = celery_app.conf.beat_schedule
print('distillation-every-30m' in schedule)
print('quality-signal-aggregation' in schedule)
print('Schedule entries:', list(schedule.keys()))
"` — both schedule entries present.
  </verify>
  <done>
    Distillation runs every 30 minutes via Celery Beat. Threshold check inside the task body prevents unnecessary work. Quality signal aggregation runs every 10 minutes. Both tasks use lazy imports to avoid loading heavy ML models in Celery worker on startup.
  </done>
</task>

</tasks>

<verification>
1. Distillation task imports and runs without errors
2. Celery Beat schedule includes both distillation (30m) and aggregation (10m)
3. Distillation short-circuits when below threshold
4. Generated summaries pass through PII pipeline
5. Merged duplicates are expired (expired_at set), not deleted
6. Provenance links (source_item_ids) maintained in tags
7. Low-quality items flagged in pre-screening
</verification>

<success_criteria>
- Distillation merges duplicates by expiring non-canonical items
- Contradictions are flagged as clusters
- Summaries generated for 3+ item clusters with PII re-scan
- Quality pre-screening marks low-quality pending contributions
- Celery Beat runs distillation every 30 minutes
- Threshold check prevents unnecessary runs
- Provenance links enable erasure propagation
</success_criteria>

<output>
After completion, create `.planning/phases/03-quality-intelligence-sdks/03-06-SUMMARY.md`
</output>
