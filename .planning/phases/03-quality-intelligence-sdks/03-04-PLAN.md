---
phase: 03-quality-intelligence-sdks
plan: 04
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - hivemind/dedup/__init__.py
  - hivemind/dedup/cosine_stage.py
  - hivemind/dedup/minhash_stage.py
  - hivemind/dedup/llm_stage.py
  - hivemind/dedup/pipeline.py
  - hivemind/conflict/__init__.py
  - hivemind/conflict/resolver.py
  - hivemind/server/tools/add_knowledge.py
autonomous: true
requirements:
  - KM-03
  - KM-07

must_haves:
  truths:
    - "Near-duplicate detection compares new content against top-10 most similar existing items using three stages"
    - "Stage 1 (cosine) rejects non-duplicates below threshold quickly"
    - "Stage 2 (MinHash LSH) catches lexical near-duplicates that embeddings miss"
    - "Stage 3 (LLM) confirms semantic duplicates above configurable threshold"
    - "Conflict resolution produces one of four outcomes: UPDATE, ADD, NOOP, VERSION_FORK"
    - "VERSION_FORK creates a sibling knowledge item with different version scope"
    - "add_knowledge runs dedup pipeline before inserting new contributions"
  artifacts:
    - path: "hivemind/dedup/pipeline.py"
      provides: "Three-stage dedup orchestration"
      exports: ["run_dedup_pipeline"]
    - path: "hivemind/dedup/minhash_stage.py"
      provides: "MinHash LSH near-duplicate detection"
      exports: ["get_lsh_index", "minhash_for_text", "find_minhash_candidates"]
    - path: "hivemind/dedup/llm_stage.py"
      provides: "LLM-based semantic duplicate confirmation"
      exports: ["confirm_duplicate_llm"]
    - path: "hivemind/conflict/resolver.py"
      provides: "LLM-assisted conflict resolution with four outcomes"
      exports: ["resolve_conflict"]
  key_links:
    - from: "hivemind/dedup/pipeline.py"
      to: "hivemind/dedup/cosine_stage.py"
      via: "calls cosine similarity as stage 1"
      pattern: "find_cosine_candidates"
    - from: "hivemind/dedup/pipeline.py"
      to: "hivemind/dedup/minhash_stage.py"
      via: "calls MinHash LSH as stage 2"
      pattern: "find_minhash_candidates"
    - from: "hivemind/dedup/pipeline.py"
      to: "hivemind/dedup/llm_stage.py"
      via: "calls LLM confirmation as stage 3"
      pattern: "confirm_duplicate_llm"
    - from: "hivemind/server/tools/add_knowledge.py"
      to: "hivemind/dedup/pipeline.py"
      via: "runs dedup before insertion"
      pattern: "run_dedup_pipeline"
---

<objective>
Build the three-stage near-duplicate detection pipeline and LLM-assisted conflict resolution.

Purpose: KM-03 requires that near-duplicate knowledge contributed by different agents is detected and consolidated rather than duplicated. KM-07 requires intelligent conflict resolution when new knowledge contradicts existing items. Together, these prevent the commons from accumulating redundant or conflicting entries.

Output: Three-stage dedup pipeline (cosine -> MinHash -> LLM), conflict resolver with UPDATE/ADD/NOOP/VERSION_FORK outcomes, and integration into the add_knowledge ingestion flow.
</objective>

<execution_context>
@/Users/amirkellousidhoum/.claude/get-shit-done/workflows/execute-plan.md
@/Users/amirkellousidhoum/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-quality-intelligence-sdks/03-RESEARCH.md

# Plan 01 creates quality/temporal schema
@.planning/phases/03-quality-intelligence-sdks/03-01-SUMMARY.md

# Existing dedup-related code
@hivemind/cli/client.py
@hivemind/server/tools/add_knowledge.py
@hivemind/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Three-stage dedup pipeline (cosine + MinHash + LLM)</name>
  <files>
    hivemind/dedup/__init__.py
    hivemind/dedup/cosine_stage.py
    hivemind/dedup/minhash_stage.py
    hivemind/dedup/llm_stage.py
    hivemind/dedup/pipeline.py
  </files>
  <action>
    1. Create `hivemind/dedup/__init__.py` — empty init file.

    2. Create `hivemind/dedup/cosine_stage.py`:
       - `async def find_cosine_candidates(content: str, org_id: str, top_k: int = 10) -> list[dict]`:
         - Embed the content using `get_embedder().embed(content)`
         - Query knowledge_items with cosine distance, org isolation (org_id OR is_public), deleted_at IS NULL, expired_at IS NULL
         - Return top_k results as list of dicts: `{ id, content, content_hash, distance, category, version }`
         - Reuse the same cosine distance pattern from search_knowledge.py (not PgVectorDriver — avoid introducing a new abstraction layer in the hot path)

    3. Create `hivemind/dedup/minhash_stage.py`:
       - Use `datasketch` library (MinHash, MinHashLSH).
       - `_lsh_index: MinHashLSH | None = None` — module-level singleton.
       - `def get_lsh_index() -> MinHashLSH`:
         Global singleton initializer. Creates `MinHashLSH(threshold=settings.minhash_threshold, num_perm=settings.minhash_num_perm)`.
         Uses lazy import of settings to avoid circular dependency.
       - `def minhash_for_text(text: str, num_perm: int = 128) -> MinHash`:
         Tokenizes text (lowercase split), updates MinHash with each token. Returns MinHash object.
       - `def insert_into_lsh(item_id: str, content: str) -> None`:
         Creates MinHash for content and inserts into the LSH index with item_id as key.
         Catches `ValueError` (duplicate key) silently — item already in index.
       - `def find_minhash_candidates(content: str) -> list[str]`:
         Queries the LSH index, returns list of item IDs with Jaccard similarity >= threshold.
       - `async def rebuild_lsh_index() -> int`:
         Drops and rebuilds the entire LSH index from all active knowledge_items.
         Queries all non-deleted, non-expired items. Returns count of items indexed.
         Use this at startup or when threshold config changes.

    4. Create `hivemind/dedup/llm_stage.py`:
       - `async def confirm_duplicate_llm(content_a: str, content_b: str) -> dict`:
         Calls the configured LLM (settings.llm_provider / settings.llm_model) to confirm whether two items are semantic duplicates.
         - Uses httpx async client to call the LLM API.
         - Prompt: "You are a deduplication assistant. Compare these two knowledge items and determine if they are semantically duplicate (same information, possibly different wording). Respond with JSON: {\"is_duplicate\": bool, \"confidence\": float, \"reason\": string}"
         - For Anthropic: POST to `https://api.anthropic.com/v1/messages` with the configured model.
         - Returns `{ is_duplicate: bool, confidence: float, reason: str }`.
         - Abstract behind an internal `_call_llm(prompt: str) -> str` function so the provider can be swapped.
         - Requires `HIVEMIND_ANTHROPIC_API_KEY` env var (add `anthropic_api_key: str = ""` to Settings).
         - If API key is empty, skip LLM stage and return `{ is_duplicate: False, confidence: 0.0, reason: "LLM stage skipped — no API key configured" }`. This makes the LLM stage optional.

    5. Create `hivemind/dedup/pipeline.py`:
       - `async def run_dedup_pipeline(content: str, org_id: str) -> dict`:
         Orchestrates the three stages:
         - **Stage 1 (Cosine):** Call `find_cosine_candidates(content, org_id, top_k=10)`.
           If no candidates with distance < 0.35 (65% similarity), return `{ action: "ADD", duplicates: [] }`.
         - **Stage 2 (MinHash):** For each cosine candidate, check `find_minhash_candidates(content)`.
           Filter candidates to those appearing in both cosine and MinHash results (intersection).
           If no intersection, return `{ action: "ADD", duplicates: cosine_candidates }` (similar but not duplicate).
         - **Stage 3 (LLM):** For each remaining candidate (max 3), call `confirm_duplicate_llm(content, candidate.content)`.
           If any confirmed duplicate with confidence >= settings.minhash_threshold: return `{ action: "DUPLICATE", duplicate_of: best_match_id, confidence: ..., duplicates: [...] }`.
         - If no confirmed duplicates after all three stages: return `{ action: "ADD", duplicates: [...] }`.
         - Return dict includes: `action` ("ADD" or "DUPLICATE"), `duplicate_of` (id if DUPLICATE), `duplicates` (all candidates found), `stages_run` (list of stage names executed).

    AVOID: Do NOT build the LSH index synchronously per-request — use the singleton pattern. The index is populated incrementally via insert_into_lsh() after each item approval.
    AVOID: Do NOT make the LLM stage blocking for the entire pipeline — if LLM API is slow/down, cosine+MinHash stages still provide value. Use a timeout of 10 seconds for the LLM call.
  </action>
  <verify>
    Run `cd /Users/amirkellousidhoum/Desktop/Code/HiveMind && python -c "
from hivemind.dedup.minhash_stage import get_lsh_index, minhash_for_text
lsh = get_lsh_index()
mh = minhash_for_text('test content for dedup')
print(f'LSH threshold: {lsh.h}')  # h is num_perm
print(f'MinHash values: {len(mh.hashvalues)}')
print('OK')
"` — should create LSH index and MinHash object.
    Run `python -c "from hivemind.dedup.pipeline import run_dedup_pipeline; print('Pipeline imported OK')"` — import succeeds.
  </verify>
  <done>
    Three-stage dedup pipeline: cosine similarity (reuses existing vector search) -> MinHash LSH (datasketch singleton) -> LLM confirmation (optional, configurable provider). Pipeline returns ADD or DUPLICATE action with confidence scores. LSH index is singleton with incremental insert support.
  </done>
</task>

<task type="auto">
  <name>Task 2: LLM conflict resolution and add_knowledge integration</name>
  <files>
    hivemind/conflict/__init__.py
    hivemind/conflict/resolver.py
    hivemind/server/tools/add_knowledge.py
  </files>
  <action>
    1. Create `hivemind/conflict/__init__.py` — empty init file.

    2. Create `hivemind/conflict/resolver.py`:
       - `async def resolve_conflict(new_content: str, existing_item: dict, org_id: str) -> dict`:
         Determines the relationship between new content and an existing near-duplicate:
         - Calls LLM with structured prompt:
           "You are a knowledge conflict resolver. Compare NEW knowledge with EXISTING knowledge and determine the appropriate action. Respond with JSON:
           {
             \"action\": \"UPDATE\" | \"ADD\" | \"NOOP\" | \"VERSION_FORK\",
             \"reason\": string,
             \"is_direct_conflict\": bool
           }
           Rules:
           - UPDATE: New knowledge supersedes existing (newer version, corrected info)
           - ADD: New knowledge is distinct enough to coexist (different angle, complementary)
           - NOOP: New knowledge adds nothing beyond existing (exact or near-exact duplicate)
           - VERSION_FORK: Both are valid but for different versions/contexts (e.g. Python 3.11 vs 3.12 behavior)
           - Only resolve DIRECT single-hop conflicts. If the conflict involves multi-hop reasoning, set is_direct_conflict=false."
         - If is_direct_conflict is false, flag for human review (add to a "conflicts_pending_review" status) and return `{ action: "FLAGGED_FOR_REVIEW", reason: ... }`.
         - If no LLM API key configured, default to ADD (let the item through — dedup will catch true duplicates).
         - Returns: `{ action, reason, is_direct_conflict, existing_item_id }`.

       - `async def apply_conflict_resolution(resolution: dict, new_content: str, existing_item_id: str, org_id: str) -> dict`:
         Applies the resolution:
         - UPDATE: Set `expired_at = now()` on the existing item (system-time invalidation), then allow new item to be inserted. Return `{ applied: "UPDATE", expired_item_id: ... }`.
         - ADD: No changes to existing — new item proceeds normally. Return `{ applied: "ADD" }`.
         - NOOP: Block the new item from being inserted. Return `{ applied: "NOOP", reason: "duplicate" }`.
         - VERSION_FORK: Set `invalid_at = now()` on existing item (world-time end), insert new item with `valid_at = now()` and the version from new content's metadata. Return `{ applied: "VERSION_FORK", sibling_id: ... }`.

    3. Update `hivemind/server/tools/add_knowledge.py`:
       - After the existing pipeline steps (auth -> injection -> burst -> PII -> hash) and BEFORE the DB insert:
         - Run dedup pipeline: `dedup_result = await run_dedup_pipeline(stripped_content, org_id)`
         - If `dedup_result["action"] == "DUPLICATE"`:
           - Run conflict resolution: `resolution = await resolve_conflict(stripped_content, dedup_result["duplicates"][0], org_id)`
           - If resolution action is NOOP: return early with `{ status: "duplicate_detected", duplicate_of: ..., reason: ... }` (not an error — informational)
           - If resolution action is UPDATE: apply resolution (expire old item), then proceed with insert
           - If resolution action is VERSION_FORK: apply resolution, then proceed with insert
           - If resolution action is ADD: proceed with insert (the items are distinct enough)
           - If resolution action is FLAGGED_FOR_REVIEW: insert as pending with a `is_conflict_flagged` note in the contribution metadata
         - If `dedup_result["action"] == "ADD"`: proceed with insert normally (no duplicates found)
       - Add `from hivemind.dedup.pipeline import run_dedup_pipeline` and `from hivemind.conflict.resolver import resolve_conflict, apply_conflict_resolution` imports (lazy inside function to avoid circular deps if needed).

    AVOID: Do NOT block add_knowledge if the LLM is unavailable — default to ADD (let item through) when LLM stage fails. The dedup pipeline already handles this gracefully.
    AVOID: Do NOT attempt multi-hop conflict resolution — per KM-07, multi-hop conflicts are explicitly flagged for human review.
  </action>
  <verify>
    Run `cd /Users/amirkellousidhoum/Desktop/Code/HiveMind && python -c "
from hivemind.conflict.resolver import resolve_conflict, apply_conflict_resolution
print('Conflict resolver imported OK')
from hivemind.dedup.pipeline import run_dedup_pipeline
print('Dedup pipeline imported OK')
"` — both imports succeed.
  </verify>
  <done>
    Conflict resolver produces UPDATE/ADD/NOOP/VERSION_FORK outcomes via LLM analysis. Multi-hop conflicts are flagged for human review. add_knowledge runs three-stage dedup before insertion and applies conflict resolution when duplicates are found. LLM unavailability defaults to ADD (non-blocking). VERSION_FORK creates sibling items with different temporal validity ranges.
  </done>
</task>

</tasks>

<verification>
1. `from hivemind.dedup.pipeline import run_dedup_pipeline` succeeds
2. `from hivemind.conflict.resolver import resolve_conflict` succeeds
3. MinHash LSH index initializes as singleton
4. LLM stage gracefully skips when no API key configured
5. add_knowledge calls dedup pipeline before insertion
6. NOOP resolution prevents duplicate insertion
7. VERSION_FORK creates temporal sibling
</verification>

<success_criteria>
- Three-stage dedup pipeline runs: cosine finds candidates, MinHash narrows, LLM confirms
- Conflict resolution returns one of four valid outcomes
- Multi-hop conflicts flagged for human review (not auto-resolved)
- add_knowledge integrates dedup+conflict as a non-blocking step
- LLM stage is optional (graceful degradation when API key missing)
</success_criteria>

<output>
After completion, create `.planning/phases/03-quality-intelligence-sdks/03-04-SUMMARY.md`
</output>
