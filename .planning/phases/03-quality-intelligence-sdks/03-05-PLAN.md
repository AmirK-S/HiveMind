---
phase: 03-quality-intelligence-sdks
plan: 05
type: execute
wave: 3
depends_on: ["03-01", "03-03"]
files_modified:
  - hivemind/server/tools/search_knowledge.py
  - hivemind/quality/aggregator.py
  - hivemind/webhooks/tasks.py
  - hivemind/server/main.py
autonomous: true
requirements:
  - KM-02
  - QI-03
  - QI-02

must_haves:
  truths:
    - "Search results are ranked by combined quality + relevance using RRF with quality boosting"
    - "Pure retrieval tier (vector + text matching + RRF) runs without LLM in the hot path"
    - "Quality signal aggregation runs as a Celery periodic task and updates quality_score on knowledge items"
    - "retrieval_count is incremented on each search hit"
  artifacts:
    - path: "hivemind/server/tools/search_knowledge.py"
      provides: "Hybrid search with quality-boosted RRF ranking"
      contains: "rrf_score"
    - path: "hivemind/quality/aggregator.py"
      provides: "Celery task for quality signal aggregation"
      exports: ["aggregate_quality_signals"]
    - path: "hivemind/webhooks/tasks.py"
      provides: "Celery Beat schedule including signal aggregation"
      contains: "quality-signal-aggregation"
  key_links:
    - from: "hivemind/server/tools/search_knowledge.py"
      to: "hivemind/db/models.py"
      via: "uses quality_score in ranking formula"
      pattern: "quality_score"
    - from: "hivemind/quality/aggregator.py"
      to: "hivemind/quality/scorer.py"
      via: "calls compute_quality_score to update items"
      pattern: "compute_quality_score"
    - from: "hivemind/webhooks/tasks.py"
      to: "hivemind/quality/aggregator.py"
      via: "schedules aggregation task in Celery Beat"
      pattern: "aggregate_quality_signals"
---

<objective>
Upgrade search to hybrid BM25+vector ranking with quality boosting, and add Celery-based quality signal aggregation.

Purpose: KM-02 requires two retrieval tiers (pure <200ms, full <1.5s). QI-03 requires search results ranked by quality combined with relevance. QI-02 requires behavioral signals to feed back into quality scores. This plan makes search quality-aware and closes the feedback loop between agent usage and ranking.

Output: Hybrid RRF search query with quality boosting in search_knowledge, quality signal aggregation Celery task on a 10-minute schedule, and retrieval count tracking.
</objective>

<execution_context>
@/Users/amirkellousidhoum/.claude/get-shit-done/workflows/execute-plan.md
@/Users/amirkellousidhoum/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-quality-intelligence-sdks/03-RESEARCH.md

# Plan 01 creates quality infrastructure, Plan 03 adds temporal + outcome reporting
@.planning/phases/03-quality-intelligence-sdks/03-01-SUMMARY.md
@.planning/phases/03-quality-intelligence-sdks/03-03-SUMMARY.md

@hivemind/server/tools/search_knowledge.py
@hivemind/webhooks/tasks.py
@hivemind/quality/scorer.py
@hivemind/quality/signals.py
@hivemind/db/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Hybrid search with RRF and quality-boosted ranking</name>
  <files>
    hivemind/server/tools/search_knowledge.py
  </files>
  <action>
    1. Upgrade `_search()` in `hivemind/server/tools/search_knowledge.py` to use hybrid retrieval with quality boosting:

       **Architecture decision on BM25:**
       Per research Open Question 1, start with PostgreSQL's built-in full-text search (`to_tsvector` / `ts_rank`) rather than pg_search or pg_textsearch. Reason: both extensions require OS-level installation on the DB host, which adds friction. PostgreSQL's native `ts_rank` is not true BM25 but is adequate for the initial implementation. The RRF fusion pattern works identically — if pg_search is installed later, only the BM25 CTE changes.

       **Implementation:**
       - Replace the single cosine-distance query with a two-CTE approach fused by RRF:

       ```python
       # CTE 1: Vector search (existing cosine distance)
       vector_cte = (
           select(
               KnowledgeItem.id,
               func.row_number().over(
                   order_by=KnowledgeItem.embedding.cosine_distance(query_embedding)
               ).label("vec_rank")
           )
           .where(org_filter, deleted_filter, embedding_filter)
           .limit(20)
       ).cte("vector_results")

       # CTE 2: Text search (PostgreSQL ts_rank)
       ts_query = func.plainto_tsquery("english", query)
       ts_vector = func.to_tsvector("english", KnowledgeItem.content)
       text_cte = (
           select(
               KnowledgeItem.id,
               func.row_number().over(
                   order_by=func.ts_rank(ts_vector, ts_query).desc()
               ).label("text_rank")
           )
           .where(org_filter, deleted_filter, ts_vector.op("@@")(ts_query))
           .limit(20)
       ).cte("text_results")

       # RRF fusion: SUM(1.0 / (60 + rank)) per item
       # Union both CTEs, group by id, sum RRF scores
       rrf_union = union_all(
           select(vector_cte.c.id, vector_cte.c.vec_rank.label("rank")),
           select(text_cte.c.id, text_cte.c.text_rank.label("rank"))
       ).subquery()

       rrf_scores = (
           select(
               rrf_union.c.id,
               func.sum(1.0 / (60 + rrf_union.c.rank)).label("rrf_score")
           )
           .group_by(rrf_union.c.id)
       ).subquery()

       # Quality-boosted final score: rrf_score * (0.7 + 0.3 * quality_score)
       final_query = (
           select(
               KnowledgeItem,
               (rrf_scores.c.rrf_score * (0.7 + 0.3 * KnowledgeItem.quality_score)).label("final_score")
           )
           .join(rrf_scores, KnowledgeItem.id == rrf_scores.c.id)
           .order_by(text("final_score DESC"))
           .limit(limit)
           .offset(offset)
       )
       ```

       - Import `func`, `text`, `union_all` from sqlalchemy as needed.
       - Apply temporal filters (from Plan 03) and category filters to both CTEs.
       - The `relevance_score` in results should now be the `final_score` (quality-boosted RRF) instead of `1 - cosine_distance`.

    2. Add retrieval count tracking:
       - After building results, collect all returned item IDs.
       - Batch increment retrieval_count: `UPDATE knowledge_items SET retrieval_count = retrieval_count + 1 WHERE id IN (:ids)`.
       - Also record retrieval signals: for each returned item, call `record_signal(item_id, "retrieval")` — but do this asynchronously (fire-and-forget) to avoid blocking the response. Use `asyncio.create_task()` wrapping a batch signal recording function.

    3. Keep backward compatibility:
       - The result shape stays identical: `{ results: [...], total_found, next_cursor }`.
       - Each result still has: id, title, category, confidence, org_attribution, relevance_score.
       - The only change is relevance_score now reflects quality-boosted RRF instead of raw cosine similarity.
       - Deduplication by content_hash continues to work identically.

    AVOID: Do NOT add pg_search or pg_textsearch dependency — use PostgreSQL's built-in to_tsvector/ts_rank for the initial implementation. This is a deliberate simplification per research Open Question 1 recommendation.
    AVOID: Do NOT compute quality boosting in Python post-processing — it must be part of the SQL query to meet the <200ms P95 target (research anti-pattern).
    AVOID: Do NOT block search response on signal recording — use fire-and-forget async task.
  </action>
  <verify>
    Run `cd /Users/amirkellousidhoum/Desktop/Code/HiveMind && python -c "
import ast, inspect
from hivemind.server.tools.search_knowledge import _search
source = inspect.getsource(_search)
print('Has rrf:', 'rrf' in source.lower())
print('Has quality_score:', 'quality_score' in source)
print('Has ts_rank or tsvector:', 'tsvector' in source.lower() or 'ts_rank' in source.lower())
"` — should show all three as True.
  </verify>
  <done>
    Search uses hybrid vector+text retrieval fused by RRF with quality_score boosting. Pure retrieval tier runs entirely in SQL (no LLM). Retrieval counts are tracked on each search. Result shape is backward compatible.
  </done>
</task>

<task type="auto">
  <name>Task 2: Quality signal aggregation Celery task with Beat schedule</name>
  <files>
    hivemind/quality/aggregator.py
    hivemind/webhooks/tasks.py
    hivemind/server/main.py
  </files>
  <action>
    1. Create `hivemind/quality/aggregator.py`:
       - `def aggregate_quality_signals() -> dict`:
         This is a Celery task function (registered via decorator in tasks.py).
         Steps:
         a. Query all knowledge_items that have received quality_signals since last aggregation.
            Use: `SELECT DISTINCT knowledge_item_id FROM quality_signals WHERE created_at > :last_run`.
            Store last_run in deployment_config table (key: "quality_aggregation_last_run").
         b. For each item, compute the updated quality score:
            - Query aggregate signal counts: helpful_count, not_helpful_count from KnowledgeItem (denormalized)
            - Query retrieval_count from KnowledgeItem
            - Compute contradiction_rate: count of "contradiction" signals / total signals
            - Compute days_since_last_access: from last retrieval signal's created_at
            - Determine is_version_current: True if no newer VERSION_FORK sibling exists
            - Call `compute_quality_score(...)` from scorer.py
         c. Update KnowledgeItem.quality_score with the computed value.
         d. Update deployment_config "quality_aggregation_last_run" to now.
         e. Return `{ items_updated: int, run_at: str }`.
         - Use sync SQLAlchemy session (same pattern as webhooks/tasks.py — Celery tasks run in sync worker).
         - Import `SessionFactory` from `hivemind.cli.client` for sync DB access.

    2. Update `hivemind/webhooks/tasks.py`:
       - Register the aggregation task:
         ```python
         @celery_app.task(name="hivemind.aggregate_quality_signals")
         def aggregate_quality_signals_task():
             from hivemind.quality.aggregator import aggregate_quality_signals
             return aggregate_quality_signals()
         ```
       - Add Celery Beat schedule in `configure_celery()`:
         ```python
         from celery.schedules import crontab
         celery_app.conf.beat_schedule = {
             "quality-signal-aggregation": {
                 "task": "hivemind.aggregate_quality_signals",
                 "schedule": crontab(minute="*/10"),  # every 10 minutes
             },
         }
         ```
       - Keep existing webhook delivery task unchanged.

    3. Update `hivemind/server/main.py` lifespan:
       - After Celery configuration, add a log line: `logger.info("Celery Beat schedule configured with quality signal aggregation.")`
       - No other changes needed — Celery Beat schedule is configured in configure_celery().

    AVOID: Do NOT query ALL knowledge_items on every aggregation run — only items with new signals since last run. This scales with signal volume, not total item count.
    AVOID: Do NOT use async session in the aggregation task — Celery workers are synchronous. Use the sync SessionFactory pattern from cli/client.py.
  </action>
  <verify>
    Run `cd /Users/amirkellousidhoum/Desktop/Code/HiveMind && python -c "
from hivemind.webhooks.tasks import celery_app
celery_app.conf.broker_url = 'redis://localhost:6379/0'
from hivemind.quality.aggregator import aggregate_quality_signals
print('Aggregator imported OK')
print('Beat schedule:', 'quality-signal-aggregation' in celery_app.conf.get('beat_schedule', {}))
"` — aggregator imports and Beat schedule is configured.
  </verify>
  <done>
    Quality signal aggregation task queries items with new signals, recomputes quality_score via weighted formula, and updates the KnowledgeItem row. Celery Beat runs the aggregation every 10 minutes. Only items with new signals are recomputed (efficient). The feedback loop is closed: agent outcomes -> signals -> aggregation -> updated quality_score -> better search ranking.
  </done>
</task>

</tasks>

<verification>
1. search_knowledge returns results ranked by quality-boosted RRF score
2. Text-matching queries (exact keywords) rank higher than pure semantic matches
3. Items with higher quality_score rank above lower-quality items at similar relevance
4. Quality signal aggregation task runs and updates quality_score
5. retrieval_count increments on search
6. Celery Beat schedule includes aggregation every 10 minutes
</verification>

<success_criteria>
- Hybrid search fuses vector + text results via RRF in SQL (no Python-side merging)
- Quality score boosts ranking: quality_boosted_score = rrf_score * (0.7 + 0.3 * quality_score)
- Signal aggregation computes scores from retrieval frequency, usefulness, contradiction rate, staleness, version freshness
- Aggregation only processes items with new signals (incremental)
- All query operations meet the sub-200ms P95 target (no LLM in the hot path)
</success_criteria>

<output>
After completion, create `.planning/phases/03-quality-intelligence-sdks/03-05-SUMMARY.md`
</output>
