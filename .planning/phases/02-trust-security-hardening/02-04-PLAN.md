---
phase: 02-trust-security-hardening
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - hivemind/graph/__init__.py
  - hivemind/graph/driver.py
  - hivemind/webhooks/__init__.py
  - hivemind/webhooks/tasks.py
autonomous: true
requirements: [INFRA-02, INFRA-03]

must_haves:
  truths:
    - "KnowledgeStoreDriver ABC defines the interface for knowledge storage backends"
    - "PgVectorDriver wraps existing pgvector operations behind the driver interface"
    - "FalkorDBDriver implements the driver interface for graph-native queries"
    - "deliver_webhook Celery task POSTs to registered webhook URLs with retry logic"
  artifacts:
    - path: "hivemind/graph/driver.py"
      provides: "KnowledgeStoreDriver ABC + PgVectorDriver + FalkorDBDriver"
      contains: "class KnowledgeStoreDriver"
    - path: "hivemind/webhooks/tasks.py"
      provides: "Celery app + deliver_webhook task"
      contains: "celery_app"
  key_links:
    - from: "hivemind/graph/driver.py"
      to: "graphiti_core"
      via: "FalkorDriver from graphiti-core[falkordb]"
      pattern: "FalkorDriver|graphiti_core"
    - from: "hivemind/webhooks/tasks.py"
      to: "celery"
      via: "Celery task decorator"
      pattern: "celery_app.task"
---

<objective>
Create the knowledge store abstraction layer following Graphiti's GraphDriver pattern and the webhook delivery infrastructure using Celery.

Purpose: INFRA-02 establishes a backend-agnostic interface so knowledge storage can be swapped (pgvector today, FalkorDB for graph queries). INFRA-03 enables near-real-time push delivery to external consumers when knowledge is approved.
Output: New hivemind/graph/ and hivemind/webhooks/ packages
</objective>

<execution_context>
@/Users/amirkellousidhoum/.claude/get-shit-done/workflows/execute-plan.md
@/Users/amirkellousidhoum/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-trust-security-hardening/02-RESEARCH.md
@hivemind/db/session.py
@hivemind/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create KnowledgeStoreDriver abstraction with PgVector and FalkorDB implementations</name>
  <files>hivemind/graph/__init__.py, hivemind/graph/driver.py</files>
  <action>
Create the `hivemind/graph/` directory and package.

**Create `hivemind/graph/__init__.py`** — empty file (package marker).

**Create `hivemind/graph/driver.py`** with:

The driver abstraction follows Graphiti's GraphDriver pattern (11 core operations) but adapted for HiveMind's knowledge domain. The key insight from research: we adopt the *interface pattern*, not replace pgvector. pgvector remains the operational store; FalkorDB augments it for graph traversal.

**`KnowledgeStoreDriver` ABC (abstract base class):**
```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any

@dataclass
class KnowledgeNode:
    """Unified knowledge representation across backends."""
    id: str
    content: str
    content_hash: str
    category: str
    org_id: str
    embedding: list[float] | None = None
    metadata: dict[str, Any] | None = None

@dataclass
class SearchResult:
    """Search result with relevance score."""
    node: KnowledgeNode
    score: float  # 0.0 to 1.0 (higher = more relevant)
```

Abstract methods on `KnowledgeStoreDriver`:
- `async def store(self, node: KnowledgeNode) -> str` — store a knowledge node, return its ID
- `async def fetch(self, node_id: str, org_id: str) -> KnowledgeNode | None` — fetch by ID with org scoping
- `async def search(self, query_embedding: list[float], org_id: str, limit: int = 10, category: str | None = None) -> list[SearchResult]` — vector similarity search
- `async def delete(self, node_id: str, org_id: str) -> bool` — soft delete, return success
- `async def verify_integrity(self, node_id: str) -> bool` — check content hash matches stored content
- `async def find_similar(self, content_embedding: list[float], org_id: str, threshold: float = 0.35, limit: int = 3) -> list[SearchResult]` — near-duplicate detection
- `async def health_check(self) -> bool` — returns True if backend is reachable

**`PgVectorDriver` implementation:**
- Constructor takes no args — uses `get_session()` from `hivemind.db.session` internally
- Implements all abstract methods by wrapping existing SQLAlchemy queries from search_knowledge.py and cli/client.py patterns
- `store()`: Creates a KnowledgeItem via SQLAlchemy
- `fetch()`: SELECT with org isolation + deleted_at check
- `search()`: Cosine distance query (same as `_search()` in search_knowledge.py)
- `delete()`: Sets deleted_at timestamp
- `verify_integrity()`: Fetches item, computes SHA-256, compares with stored content_hash
- `find_similar()`: Cosine distance top-N with threshold (same as `find_similar_knowledge()` in cli/client.py)
- `health_check()`: `SELECT 1` query

**`FalkorDBDriver` implementation (skeleton):**
- Constructor takes `host: str, port: int, database: str`
- Lazy import: `from graphiti_core.driver.falkordb_driver import FalkorDriver` inside `__init__`
- Creates `FalkorDriver(host=host, port=port, database=database)` as `self._driver`
- All abstract methods raise `NotImplementedError("FalkorDB driver not yet fully implemented — use PgVectorDriver")` for now EXCEPT:
  - `health_check()`: Attempts a ping to FalkorDB and returns True/False
- This is a scaffold — full FalkorDB implementation is Phase 3 work

**Module-level factory:**
`def get_driver(backend: str = "pgvector") -> KnowledgeStoreDriver`:
- Returns `PgVectorDriver()` if backend == "pgvector"
- Returns `FalkorDBDriver(...)` with settings from config if backend == "falkordb"
- Raises `ValueError` for unknown backend

Docstring references INFRA-02 and explains the abstraction pattern.
  </action>
  <verify>
Run `python -c "from hivemind.graph.driver import KnowledgeStoreDriver, PgVectorDriver, FalkorDBDriver, get_driver, KnowledgeNode, SearchResult; print('Graph driver module importable')"` — should succeed.
Run `python -c "from hivemind.graph.driver import get_driver; d = get_driver('pgvector'); print(f'Driver type: {type(d).__name__}')"` — should print "PgVectorDriver".
  </verify>
  <done>KnowledgeStoreDriver ABC with 7 abstract methods exists. PgVectorDriver wraps existing pgvector queries. FalkorDBDriver is scaffolded with health_check only. get_driver() factory selects backend by name.</done>
</task>

<task type="auto">
  <name>Task 2: Create Celery webhook delivery infrastructure</name>
  <files>hivemind/webhooks/__init__.py, hivemind/webhooks/tasks.py</files>
  <action>
Create the `hivemind/webhooks/` directory and package.

**Create `hivemind/webhooks/__init__.py`** — empty file (package marker).

**Create `hivemind/webhooks/tasks.py`** with:

**Celery app configuration:**
```python
from celery import Celery

celery_app = Celery("hivemind")

def configure_celery(redis_url: str) -> None:
    """Configure Celery broker and result backend. Call during server lifespan."""
    celery_app.conf.broker_url = redis_url
    celery_app.conf.result_backend = redis_url
    celery_app.conf.task_serializer = "json"
    celery_app.conf.accept_content = ["json"]
```

**`deliver_webhook` Celery task:**
```python
@celery_app.task(bind=True, max_retries=3, default_retry_delay=5)
def deliver_webhook(self, webhook_url: str, payload: dict) -> dict:
    """POST knowledge event to a single webhook endpoint.

    Retries up to 3 times with 5-second delay on failure.
    Uses httpx synchronous client (Celery tasks are sync).

    Args:
        webhook_url: The URL to POST to.
        payload: JSON-serializable event payload containing:
            - event: event type string (e.g. "knowledge.approved")
            - knowledge_item_id: UUID string
            - org_id: organisation namespace
            - category: knowledge category
            - timestamp: ISO 8601 timestamp

    Returns:
        Dict with status_code and url on success.
    """
    import httpx
    try:
        with httpx.Client(timeout=10.0) as client:
            response = client.post(webhook_url, json=payload)
            response.raise_for_status()
            return {"status_code": response.status_code, "url": webhook_url}
    except Exception as exc:
        raise self.retry(exc=exc)
```

**`dispatch_webhooks` helper (called from approval flow):**
```python
def dispatch_webhooks(
    org_id: str,
    event: str,
    knowledge_item_id: str,
    category: str,
) -> int:
    """Dispatch webhook delivery tasks for all active endpoints in an org.

    Uses sync SQLAlchemy session (same pattern as cli/client.py) because
    this is called from the CLI approval flow which is synchronous.

    Args:
        org_id: Organisation namespace.
        event: Event type string (e.g. "knowledge.approved").
        knowledge_item_id: UUID string of the approved item.
        category: Knowledge category value.

    Returns:
        Number of webhook tasks dispatched.
    """
    import datetime
    from hivemind.db.models import WebhookEndpoint
    # Use the same sync SessionFactory pattern as cli/client.py
    from hivemind.cli.client import SessionFactory

    with SessionFactory() as session:
        endpoints = (
            session.query(WebhookEndpoint)
            .filter(
                WebhookEndpoint.org_id == org_id,
                WebhookEndpoint.is_active == True,  # noqa: E712
            )
            .all()
        )

    payload = {
        "event": event,
        "knowledge_item_id": knowledge_item_id,
        "org_id": org_id,
        "category": category,
        "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(),
    }

    dispatched = 0
    for endpoint in endpoints:
        # Optionally filter by event_types if configured
        if endpoint.event_types:
            event_list = endpoint.event_types.get("types", [])
            if event_list and event not in event_list:
                continue
        deliver_webhook.delay(endpoint.url, payload)
        dispatched += 1

    return dispatched
```

Include module docstring referencing INFRA-03 and explaining the near-real-time push pattern.
  </action>
  <verify>
Run `python -c "from hivemind.webhooks.tasks import celery_app, deliver_webhook, dispatch_webhooks, configure_celery; print(f'Celery app: {celery_app.main}')"` — should print "hivemind".
  </verify>
  <done>Celery app is configured with JSON serialization. deliver_webhook task POSTs to webhook URLs with 3 retries. dispatch_webhooks helper fetches active endpoints for an org and queues delivery tasks for each. All code is importable and ready for integration.</done>
</task>

</tasks>

<verification>
1. `hivemind/graph/` package exists with driver.py containing ABC + two implementations
2. `hivemind/webhooks/` package exists with tasks.py containing Celery app + webhook task
3. KnowledgeStoreDriver has 7 abstract methods
4. PgVectorDriver implements all 7 methods using existing SQLAlchemy patterns
5. FalkorDBDriver is scaffolded with NotImplementedError except health_check
6. deliver_webhook task has retry logic (max_retries=3, default_retry_delay=5)
</verification>

<success_criteria>
- Graph driver abstraction follows Graphiti's pattern with backend-agnostic interface
- PgVectorDriver wraps existing pgvector queries behind the ABC
- FalkorDBDriver scaffold is ready for Phase 3 implementation
- Celery webhook task delivers events to registered endpoints with retry
- dispatch_webhooks helper queries active endpoints and queues delivery
</success_criteria>

<output>
After completion, create `.planning/phases/02-trust-security-hardening/02-04-SUMMARY.md`
</output>
